{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "83046639-0e0f-4e18-b87e-6ca2a09f9a61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T01:36:04.557470Z",
     "iopub.status.busy": "2026-02-22T01:36:04.557002Z",
     "iopub.status.idle": "2026-02-22T01:36:06.242800Z",
     "shell.execute_reply": "2026-02-22T01:36:06.241643Z",
     "shell.execute_reply.started": "2026-02-22T01:36:04.557433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.11/site-packages (1.40.70)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.3.3)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.11/site-packages (19.0.1)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.70 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.40.70)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.1.0)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from boto3) (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.41.0,>=1.40.70->boto3) (1.26.20)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install boto3 pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "execution_state": "idle",
   "id": "ea454bb7-d655-4c6a-8633-f6243c36398b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T01:50:43.591725Z",
     "iopub.status.busy": "2026-02-22T01:50:43.591429Z",
     "iopub.status.idle": "2026-02-22T01:50:44.287109Z",
     "shell.execute_reply": "2026-02-22T01:50:44.286117Z",
     "shell.execute_reply.started": "2026-02-22T01:50:43.591701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW object size (bytes): 51841\n",
      "Raw shape: (1000, 7)\n",
      "Raw columns: ['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
      "         Date Symbol    Open    High     Low   Close    Volume\n",
      "0  2024-01-01   AAPL  154.72  161.22  151.89  159.55   3494467\n",
      "1  2024-01-02   MSFT  157.29  159.06  147.59  150.54   2138775\n",
      "2  2024-01-03   AAPL  142.91  144.99  138.40  141.21  10960425\n",
      "After type casting shape: (1000, 7)\n",
      "        date symbol    open    high     low   close    volume\n",
      "0 2024-01-01   AAPL  154.72  161.22  151.89  159.55   3494467\n",
      "1 2024-01-02   MSFT  157.29  159.06  147.59  150.54   2138775\n",
      "2 2024-01-03   AAPL  142.91  144.99  138.40  141.21  10960425\n",
      "Rows with non-null date/symbol: 1000\n",
      "Rows with non-null OHLCV: 1000\n",
      "Dropped 0 rows due to null volume\n",
      "Final cleaned shape: (1000, 7)\n",
      "        date symbol    open    high     low   close    volume\n",
      "0 2024-01-01   AAPL  154.72  161.22  151.89  159.55   3494467\n",
      "1 2024-01-03   AAPL  142.91  144.99  138.40  141.21  10960425\n",
      "2 2024-01-04   AAPL  183.98  189.54  182.29  186.80   6915432\n",
      "3 2024-01-14   AAPL  357.31  361.14  349.48  350.06   7535315\n",
      "4 2024-01-15   AAPL  102.33  108.73   98.64  107.00  11891970\n",
      "Wrote CSV to: s3://bucket2-curated-stock/curated/stock_market/stock_clean.csv\n",
      "head_object error: An error occurred (403) when calling the HeadObject operation: Forbidden\n",
      "Curated object size (bytes): None\n"
     ]
    }
   ],
   "source": [
    "import io, boto3, pandas as pd\n",
    "\n",
    "RAW_BUCKET=\"bucket1-raw-stock\"\n",
    "RAW_KEY = \"StockMarket.csv\"     # exact key (case-sensitive)\n",
    "CURATED_BUCKET = \"bucket2-curated-stock\"\n",
    "CURATED_PREFIX = \"curated/stock_market/\"\n",
    "CSV_KEY = f\"{CURATED_PREFIX}stock_clean.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def head_size(bucket, key):\n",
    "    try:\n",
    "        h = s3.head_object(Bucket=bucket, Key=key)\n",
    "        return h.get(\"ContentLength\", 0)\n",
    "    except Exception as e:\n",
    "        print(\"head_object error:\", e)\n",
    "        return None\n",
    "\n",
    "print(\"RAW object size (bytes):\", head_size(RAW_BUCKET, RAW_KEY))\n",
    "\n",
    "# -------- READ with robust delimiter detection ----------\n",
    "raw_obj = s3.get_object(Bucket=RAW_BUCKET, Key=RAW_KEY)\n",
    "raw_bytes = raw_obj[\"Body\"].read()\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_csv(io.BytesIO(raw_bytes), engine=\"python\", sep=None)\n",
    "except Exception:\n",
    "    try:\n",
    "        df_raw = pd.read_csv(io.BytesIO(raw_bytes), sep=\"\\t\")\n",
    "    except Exception:\n",
    "        df_raw = pd.read_csv(io.BytesIO(raw_bytes))  # default comma\n",
    "\n",
    "print(\"Raw shape:\", df_raw.shape)\n",
    "print(\"Raw columns:\", list(df_raw.columns))\n",
    "print(df_raw.head(3))\n",
    "\n",
    "# ---------- CLEAN: make parsing tolerant ----------\n",
    "df = df_raw.copy()\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "# Date: try strict dd-MM-yyyy first, then fall back to general parser\n",
    "if \"date\" in df.columns:\n",
    "    d1 = pd.to_datetime(df[\"date\"], format=\"%d-%m-%Y\", errors=\"coerce\")\n",
    "    if d1.notna().sum() == 0:\n",
    "        # try year-first\n",
    "        d2 = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "        if d2.notna().sum() == 0:\n",
    "            # final fallback: infer\n",
    "            d3 = pd.to_datetime(df[\"date\"], dayfirst=True, errors=\"coerce\")\n",
    "            df[\"date\"] = d3\n",
    "        else:\n",
    "            df[\"date\"] = d2\n",
    "    else:\n",
    "        df[\"date\"] = d1\n",
    "\n",
    "for c in [\"open\",\"high\",\"low\",\"close\",\"volume\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "print(\"After type casting shape:\", df.shape)\n",
    "print(df.head(3))\n",
    "\n",
    "# Check how many rows would be dropped by each rule\n",
    "def count_notna(cols):\n",
    "    return df.dropna(subset=[c for c in cols if c in df.columns]).shape[0]\n",
    "\n",
    "print(\"Rows with non-null date/symbol:\", count_notna([\"date\",\"symbol\"]))\n",
    "print(\"Rows with non-null OHLCV:\", count_notna([\"open\",\"high\",\"low\",\"close\",\"volume\"]))\n",
    "\n",
    "# Apply *minimal* required filters first\n",
    "required = [c for c in [\"date\",\"symbol\",\"open\",\"high\",\"low\",\"close\"] if c in df.columns]\n",
    "df = df.dropna(subset=required)\n",
    "\n",
    "# Only drop rows with missing VOLUME if the column exists and is critical for you\n",
    "if \"volume\" in df.columns:\n",
    "    before_vol = df.shape[0]\n",
    "    df = df.dropna(subset=[\"volume\"])\n",
    "    print(f\"Dropped {before_vol - df.shape[0]} rows due to null volume\")\n",
    "\n",
    "# Sanity checks – but don’t be overly strict for synthetic data\n",
    "if set([\"low\",\"high\",\"open\",\"close\"]).issubset(df.columns):\n",
    "    mask_ok = (\n",
    "        (df[\"low\"] <= df[\"high\"]) &\n",
    "        (df[\"open\"] >= df[\"low\"]) & (df[\"open\"] <= df[\"high\"]) &\n",
    "        (df[\"close\"] >= df[\"low\"]) & (df[\"close\"] <= df[\"high\"])\n",
    "    )\n",
    "    removed = (~mask_ok).sum()\n",
    "    if removed > 0:\n",
    "        print(f\"Filter will remove {removed} rows failing price sanity checks\")\n",
    "    # If it removes *everything*, relax the rule\n",
    "    if mask_ok.sum() == 0:\n",
    "        print(\"Warning: sanity check would remove all rows – skipping this filter.\")\n",
    "    else:\n",
    "        df = df[mask_ok]\n",
    "\n",
    "df = df.drop_duplicates().sort_values([\"symbol\",\"date\"]).reset_index(drop=True)\n",
    "print(\"Final cleaned shape:\", df.shape)\n",
    "print(df.head(5))\n",
    "\n",
    "# Abort early if empty so you don’t write an empty CSV\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"Cleaned DataFrame is empty – check date format, numeric coercion, and filters above.\")\n",
    "\n",
    "# -------- WRITE CSV ----------\n",
    "buf = io.StringIO()\n",
    "df.to_csv(buf, index=False)\n",
    "s3.put_object(Bucket=CURATED_BUCKET, Key=CSV_KEY, Body=buf.getvalue().encode(\"utf-8\"))\n",
    "\n",
    "print(\"Wrote CSV to:\", f\"s3://{CURATED_BUCKET}/{CSV_KEY}\")\n",
    "print(\"Curated object size (bytes):\", head_size(CURATED_BUCKET, CSV_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "execution_state": "idle",
   "id": "cbd788b4-d1a9-453d-bbe3-6a6e97b899b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T02:16:09.634886Z",
     "iopub.status.busy": "2026-02-22T02:16:09.634634Z",
     "iopub.status.idle": "2026-02-22T02:16:09.753516Z",
     "shell.execute_reply": "2026-02-22T02:16:09.752813Z",
     "shell.execute_reply.started": "2026-02-22T02:16:09.634859Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import uuid\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def write_csv_to_s3(df: pd.DataFrame, bucket: str, key: str) -> None:\n",
    "    \"\"\"Write a pandas DataFrame as CSV to S3 (single object).\"\"\"\n",
    "    buf = io.StringIO()\n",
    "    df.to_csv(buf, index=False)\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=buf.getvalue().encode(\"utf-8\"))\n",
    "    print(f\"[CSV] s3://{bucket}/{key}\")\n",
    "\n",
    "def write_parquet_to_s3_single(df: pd.DataFrame, bucket: str, key: str, compression: str = \"snappy\") -> None:\n",
    "    \"\"\"Write a pandas DataFrame as a single Parquet object to S3.\"\"\"\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    buf = io.BytesIO()\n",
    "    pq.write_table(table, buf, compression=compression)\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=buf.getvalue(), ContentType=\"application/octet-stream\")\n",
    "    print(f\"[PARQUET] s3://{bucket}/{key}\")\n",
    "\n",
    "def write_parquet_to_s3_sharded_by_symbol(\n",
    "    df: pd.DataFrame,\n",
    "    bucket: str,\n",
    "    prefix: str,\n",
    "    compression: str = \"snappy\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Write multiple Parquet objects to S3, one per symbol, under:\n",
    "    s3://<bucket>/<prefix>/symbol=<SYM>/part-<uuid>.parquet\n",
    "    \"\"\"\n",
    "    if \"symbol\" not in df.columns:\n",
    "        raise ValueError(\"Column 'symbol' is required for sharded write.\")\n",
    "\n",
    "    for sym, g in df.groupby(\"symbol\"):\n",
    "        g2 = g.reset_index(drop=True)\n",
    "        table = pa.Table.from_pandas(g2, preserve_index=False)\n",
    "        buf = io.BytesIO()\n",
    "        pq.write_table(table, buf, compression=compression)\n",
    "        key = f\"{prefix.rstrip('/')}/symbol={sym}/part-{uuid.uuid4().hex}.parquet\"\n",
    "        s3.put_object(Bucket=bucket, Key=key, Body=buf.getvalue(), ContentType=\"application/octet-stream\")\n",
    "        print(f\"[PARQUET SHARD] s3://{bucket}/{key}\")\n",
    "\n",
    "def simple_stock_transforms(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a couple of useful columns:\n",
    "      - range = high - low\n",
    "      - daily_return_pct = (close / open - 1) * 100\n",
    "    Sort by symbol,date for stable files.\n",
    "    \"\"\"\n",
    "    req = {\"open\", \"high\", \"low\", \"close\"}\n",
    "    if not req.issubset(df.columns):\n",
    "        missing = req - set(df.columns)\n",
    "        raise ValueError(f\"Missing required columns for transforms: {missing}\")\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"range\"] = out[\"high\"] - out[\"low\"]\n",
    "    out[\"daily_return_pct\"] = (out[\"close\"] / out[\"open\"] - 1.0) * 100.0\n",
    "    # Optional ordering\n",
    "    cols = [\"symbol\", \"date\"]\n",
    "    out = out.sort_values([c for c in cols if c in out.columns]).reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "execution_state": "idle",
   "id": "a90456cf-fad9-4fbd-87dc-d8c9bbba00c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T02:16:39.222601Z",
     "iopub.status.busy": "2026-02-22T02:16:39.222290Z",
     "iopub.status.idle": "2026-02-22T02:16:39.589708Z",
     "shell.execute_reply": "2026-02-22T02:16:39.588626Z",
     "shell.execute_reply.started": "2026-02-22T02:16:39.222560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CSV] s3://bucket2-curated-stock/curated/stock_market/stock_clean.csv\n",
      "[PARQUET] s3://bucket2-curated-stock/curated/stock_market/stock_clean.parquet\n",
      "Done writing CSV and Parquet to curated.\n"
     ]
    }
   ],
   "source": [
    "# ====== after your cleaning code produced df (non-empty) ======\n",
    "\n",
    "# 1) Apply simple transformations\n",
    "df_tx = simple_stock_transforms(df)\n",
    "\n",
    "# 2) Write CSV (same as you already do)\n",
    "CSV_KEY = f\"{CURATED_PREFIX}stock_clean.csv\"\n",
    "write_csv_to_s3(df_tx, CURATED_BUCKET, CSV_KEY)\n",
    "\n",
    "# 3A) Write a single Parquet file alongside the CSV\n",
    "PARQUET_KEY = f\"{CURATED_PREFIX}stock_clean.parquet\"\n",
    "write_parquet_to_s3_single(df_tx, CURATED_BUCKET, PARQUET_KEY)\n",
    "\n",
    "# 3B) Alternatively, write sharded Parquet by symbol (comment out if not needed)\n",
    "# This creates pseudo-partitions like curated/stock_market/symbol=AAPL/part-xxxx.parquet\n",
    "# write_parquet_to_s3_sharded_by_symbol(df_tx, CURATED_BUCKET, CURATED_PREFIX)\n",
    "\n",
    "print(\"Done writing CSV and Parquet to curated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac1216-778e-4da0-ba5b-60e0b8049504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
